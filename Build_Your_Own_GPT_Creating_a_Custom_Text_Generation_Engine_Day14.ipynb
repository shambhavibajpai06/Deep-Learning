{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Task: Code-Focused Inference**\n",
        "Your task is to load a pre-trained GPT-2 model and configure it to answer only questions related to Python coding.\n",
        "\n",
        "- Load Model and Tokenizer: Load a suitable pre-trained GPT-2 model and its corresponding tokenizer. You can use transformers.AutoModelForCausalLM and transformers.AutoTokenizer. A smaller model like gpt2 or gpt2-medium might be sufficient.\n",
        "- Implement a Filtering Mechanism: Before generating a response, check if the input prompt is related to Python coding. You can use simple keyword matching (e.g., \"Python\", \"code\", \"function\", \"class\", \"import\") or a more sophisticated approach using a text classification model (optional).\n",
        "- Generate Response: If the prompt is deemed a Python coding question, generate a response using the loaded GPT-2 model.\n",
        "- Handle Non-Coding Questions: If the prompt is not related to Python coding, return a predefined message indicating that the model can only answer coding questions.\n",
        "- Test: Test your implementation with various prompts, including both Python coding questions and non-coding questions, to ensure the filtering mechanism works correctly."
      ],
      "metadata": {
        "id": "HPoOFQ3iHRjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Define the class\n",
        "class PythonCodeGPT:\n",
        "    def __init__(self, model_name='gpt2-medium'):\n",
        "        print(f\"Loading model: {model_name}...\")\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "        # GPT-2 has no pad_token → set eos_token as padding\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Keywords for filtering Python-related questions\n",
        "        self.python_keywords = [\n",
        "            'python', 'code', 'function', 'class', 'import', 'list',\n",
        "            'dict', 'tuple', 'pandas', 'numpy', 'def', 'return', 'for',\n",
        "            'while', 'try', 'except', 'lambda', 'pip'\n",
        "        ]\n",
        "        print(f\"✅ Model loaded successfully on device: {self.device}\")\n",
        "\n",
        "    # Filtering mechanism\n",
        "    def _is_python_question(self, prompt: str) -> bool:\n",
        "        prompt_lower = prompt.lower()\n",
        "        return any(keyword in prompt_lower for keyword in self.python_keywords)\n",
        "\n",
        "    # Response generation\n",
        "    def generate_response(self, prompt: str, max_length: int = 100) -> str:\n",
        "        if not self._is_python_question(prompt):\n",
        "            return \"⚠️ I can only answer questions about Python coding questions.\"\n",
        "\n",
        "        inputs = self.tokenizer.encode(\n",
        "            prompt,\n",
        "            return_tensors='pt'\n",
        "        ).to(self.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "\n",
        "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_text\n",
        "\n"
      ],
      "metadata": {
        "id": "a4WkmytWHR0x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "BWVSlZyGKXK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test the implementation\n",
        "if __name__ == \"__main__\":\n",
        "    bot = PythonCodeGPT(\"gpt2\")\n",
        "\n",
        "    test_prompts = [\n",
        "        \"How do I write a Python function to reverse a string?\",\n",
        "        \"Tell me about the history of World War II.\",\n",
        "        \"Write Python code to read a file line by line.\",\n",
        "        \"What is the capital of France?\",\n",
        "    ]\n",
        "\n",
        "    for prompt in test_prompts:\n",
        "        print(f\"User: {prompt}\")\n",
        "        print(f\"Model: {bot.generate_response(prompt)}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W23N8-DsHU-v",
        "outputId": "63bd9ed0-ac7d-46b4-f07c-12654af1b0ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: gpt2...\n",
            "✅ Model loaded successfully on device: cpu\n",
            "User: How do I write a Python function to reverse a string?\n",
            "Model: How do I write a Python function to reverse a string?\n",
            "\n",
            "The Python interpreter will automatically convert a number of strings into a sequence of numbers. The Python string is then converted into an integer.\n",
            ".py -f -o string.txt\n",
            ",\n",
            " (python string)\n",
            "A Python program will convert the string into the integer it is intended to represent. This is called a reverse string conversion. If you want to convert an object to a different string, you can use the -i\n",
            "\n",
            "User: Tell me about the history of World War II.\n",
            "Model: ⚠️ I can only answer questions about Python coding questions.\n",
            "\n",
            "User: Write Python code to read a file line by line.\n",
            "Model: Write Python code to read a file line by line.\n",
            "\n",
            "The following example shows how to write Python to a Python file. The code is written in Python 2.7. It is not yet fully supported by Python 3.5. If you are using Python 4.x, you will need to upgrade to Python 5.0. You can find the latest version of Python on the Python Wiki.\n",
            "\n",
            "User: What is the capital of France?\n",
            "Model: ⚠️ I can only answer questions about Python coding questions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "- General LMs are not Code Experts\n",
        "\n",
        "- GPT-2 is a general-purpose model trained on text, not coding tasks.\n",
        "\n",
        "- That’s why it tends to return the question or generate rambling text instead of clean Python code.\n"
      ],
      "metadata": {
        "id": "xPrENjN-I-6Q"
      }
    }
  ]
}